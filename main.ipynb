{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.10.13","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"},"kaggle":{"accelerator":"gpu","dataSources":[{"sourceId":8400099,"sourceType":"datasetVersion","datasetId":4997851},{"sourceId":47366,"sourceType":"modelInstanceVersion","isSourceIdPinned":true,"modelInstanceId":39653}],"dockerImageVersionId":30699,"isInternetEnabled":true,"language":"python","sourceType":"notebook","isGpuEnabled":true}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"!pip install sentencepiece","metadata":{"execution":{"iopub.status.busy":"2024-05-13T16:16:30.399814Z","iopub.execute_input":"2024-05-13T16:16:30.400262Z","iopub.status.idle":"2024-05-13T16:16:43.500086Z","shell.execute_reply.started":"2024-05-13T16:16:30.400225Z","shell.execute_reply":"2024-05-13T16:16:43.498823Z"},"trusted":true},"execution_count":22,"outputs":[{"name":"stdout","text":"Requirement already satisfied: sentencepiece in /opt/conda/lib/python3.10/site-packages (0.2.0)\n","output_type":"stream"}]},{"cell_type":"code","source":"import torch\nimport torch.nn as nn\nfrom torch.nn import functional as F \n\nimport sentencepiece as spm","metadata":{"execution":{"iopub.status.busy":"2024-05-13T16:27:35.340857Z","iopub.execute_input":"2024-05-13T16:27:35.341228Z","iopub.status.idle":"2024-05-13T16:27:35.345853Z","shell.execute_reply.started":"2024-05-13T16:27:35.341198Z","shell.execute_reply":"2024-05-13T16:27:35.344964Z"},"trusted":true},"execution_count":63,"outputs":[]},{"cell_type":"code","source":"# # for TPU\n# import torch_xla\n# import torch_xla.core.xla_model as xm\n\n# DEVICE = xm.xla_device()\n# torch.set_default_tensor_type('torch.FloatTensor')","metadata":{"execution":{"iopub.status.busy":"2024-05-13T16:27:37.228031Z","iopub.execute_input":"2024-05-13T16:27:37.228353Z","iopub.status.idle":"2024-05-13T16:27:37.232349Z","shell.execute_reply.started":"2024-05-13T16:27:37.228328Z","shell.execute_reply":"2024-05-13T16:27:37.231305Z"},"trusted":true},"execution_count":64,"outputs":[]},{"cell_type":"code","source":"\nBATCH_SIZE = 64\nCONTEXT_SIZE:int = int(256)\nMAX_EPOCHS = 2500\nEVAL_EVERY = 500\nLEARNING_RATE = 3e-4\nDEVICE = 'cuda' if torch.cuda.is_available() else 'cpu'\nprint(DEVICE)\nprint(torch.cuda.memory_allocated())\nEVAL_ITERS = 200\nEMBEDDING_SIZE = 256\nNUM_HEADS = 4\nNUM_TRAN = 6\nDROPOUT = 0.2\n\nwith open('/kaggle/input/shakespeare/input.txt', 'r', encoding='utf-8') as f:\n    text = f.read()\ntext[:1000]","metadata":{"execution":{"iopub.status.busy":"2024-05-13T16:31:03.876139Z","iopub.execute_input":"2024-05-13T16:31:03.876827Z","iopub.status.idle":"2024-05-13T16:31:03.888432Z","shell.execute_reply.started":"2024-05-13T16:31:03.876793Z","shell.execute_reply":"2024-05-13T16:31:03.887552Z"},"trusted":true},"execution_count":90,"outputs":[{"name":"stdout","text":"cuda\n144952832\n","output_type":"stream"},{"execution_count":90,"output_type":"execute_result","data":{"text/plain":"\"First Citizen:\\nBefore we proceed any further, hear me speak.\\n\\nAll:\\nSpeak, speak.\\n\\nFirst Citizen:\\nYou are all resolved rather to die than to famish?\\n\\nAll:\\nResolved. resolved.\\n\\nFirst Citizen:\\nFirst, you know Caius Marcius is chief enemy to the people.\\n\\nAll:\\nWe know't, we know't.\\n\\nFirst Citizen:\\nLet us kill him, and we'll have corn at our own price.\\nIs't a verdict?\\n\\nAll:\\nNo more talking on't; let it be done: away, away!\\n\\nSecond Citizen:\\nOne word, good citizens.\\n\\nFirst Citizen:\\nWe are accounted poor citizens, the patricians good.\\nWhat authority surfeits on would relieve us: if they\\nwould yield us but the superfluity, while it were\\nwholesome, we might guess they relieved us humanely;\\nbut they think we are too dear: the leanness that\\nafflicts us, the object of our misery, is as an\\ninventory to particularise their abundance; our\\nsufferance is a gain to them Let us revenge this with\\nour pikes, ere we become rakes: for the gods know I\\nspeak this in hunger for bread, not in thirst for revenge.\\n\\n\""},"metadata":{}}]},{"cell_type":"code","source":"# # SentencePiece model\n# sp = spm.SentencePieceProcessor()\n# sp.load(\"/kaggle/input/movie-corpus_8000/other/first/1/movie-corpus_8000.model\")  # Replace \"path_to_your_model.model\" with the path to your model file\n\n\n# vocab_size = sp.get_piece_size()\n# encode = lambda s: sp.encode_as_ids(s)\n# decode = lambda l: sp.decode_ids(l)\n\n# character by character tokenization\nchars = sorted(list(set(text)))\nprint(\"Chars:\", \"\".join(chars))\nvocab_size = len(chars)\nprint(vocab_size)\n# create a mapping from characters to integers\nstoi = { ch:i for i,ch in enumerate(chars) }\nitos = { i:ch for i,ch in enumerate(chars) }\nencode = lambda s: [stoi[c] for c in s] # encoder: take a string, output a list of integers\ndecode = lambda l: ''.join([itos[i] for i in l]) # decoder: take a list of integers, output a string\n\nprint(encode(\"This is a test of the encoding\"))\nprint(encode(decode(encode(\"This is a test of the encoding\"))))","metadata":{"execution":{"iopub.status.busy":"2024-05-13T16:31:04.006282Z","iopub.execute_input":"2024-05-13T16:31:04.006803Z","iopub.status.idle":"2024-05-13T16:31:04.029771Z","shell.execute_reply.started":"2024-05-13T16:31:04.006776Z","shell.execute_reply":"2024-05-13T16:31:04.028900Z"},"trusted":true},"execution_count":91,"outputs":[{"name":"stdout","text":"Chars: \n !$&',-.3:;?ABCDEFGHIJKLMNOPQRSTUVWXYZabcdefghijklmnopqrstuvwxyz\n65\n[32, 46, 47, 57, 1, 47, 57, 1, 39, 1, 58, 43, 57, 58, 1, 53, 44, 1, 58, 46, 43, 1, 43, 52, 41, 53, 42, 47, 52, 45]\n[32, 46, 47, 57, 1, 47, 57, 1, 39, 1, 58, 43, 57, 58, 1, 53, 44, 1, 58, 46, 43, 1, 43, 52, 41, 53, 42, 47, 52, 45]\n","output_type":"stream"}]},{"cell_type":"code","source":"data = torch.tensor(encode(text), dtype=torch.long)\nprint(data[0:5])\ntrain_len = int(0.9*len(data))\ntrain_data = data[:train_len]\nprint(train_data[0:5])\ntest_data = data[train_len:]\n\ndef get_batch(split):\n    # generate a small batch of data of inputs x and targets y\n    data = train_data if split == 'train' else test_data\n    ix = torch.randint(len(data) - CONTEXT_SIZE, (BATCH_SIZE,))\n    x = torch.stack([data[i:i+CONTEXT_SIZE] for i in ix])\n    y = torch.stack([data[i+1:i+CONTEXT_SIZE+1] for i in ix])\n    x, y = x.to(DEVICE), y.to(DEVICE)\n    return x, y\n\nget_batch('train')[0]","metadata":{"execution":{"iopub.status.busy":"2024-05-13T16:31:04.165042Z","iopub.execute_input":"2024-05-13T16:31:04.165353Z","iopub.status.idle":"2024-05-13T16:31:04.423437Z","shell.execute_reply.started":"2024-05-13T16:31:04.165328Z","shell.execute_reply":"2024-05-13T16:31:04.422359Z"},"trusted":true},"execution_count":92,"outputs":[{"name":"stdout","text":"tensor([18, 47, 56, 57, 58])\ntensor([18, 47, 56, 57, 58])\n","output_type":"stream"},{"execution_count":92,"output_type":"execute_result","data":{"text/plain":"tensor([[56, 57, 58,  ..., 46, 39, 60],\n        [60, 43,  6,  ..., 43, 56,  1],\n        [ 0, 40, 56,  ..., 42, 43, 56],\n        ...,\n        [43,  1, 58,  ..., 52, 45, 12],\n        [ 1, 51, 43,  ..., 59, 56,  1],\n        [42, 11,  0,  ..., 53, 52, 43]], device='cuda:0')"},"metadata":{}}]},{"cell_type":"code","source":"## TODO: Estimate Loss Function\n\n@torch.no_grad()\ndef estimate_losses(model):\n    out = {}\n    model.eval()\n    for split in [\"train\", \"val\"]:\n        losses = torch.zeros(EVAL_ITERS)\n        for k in range(EVAL_ITERS):\n            X, Y = get_batch(split)\n            logits, loss = model(X, Y)\n            losses[k] = loss.item()\n        out[split] = losses.mean()\n    model.train()\n    return out\n    ","metadata":{"execution":{"iopub.status.busy":"2024-05-13T16:31:04.425159Z","iopub.execute_input":"2024-05-13T16:31:04.425454Z","iopub.status.idle":"2024-05-13T16:31:04.431471Z","shell.execute_reply.started":"2024-05-13T16:31:04.425428Z","shell.execute_reply":"2024-05-13T16:31:04.430619Z"},"trusted":true},"execution_count":93,"outputs":[]},{"cell_type":"code","source":"# Single Attention Head\nclass SingleHeadedAttention(nn.Module):\n    \n    def __init__(self, emb_size, head_size, context_size:int, dropout=0.2):\n        super().__init__()\n        self.key = nn.Linear(emb_size, head_size, bias=False)\n        self.query = nn.Linear(emb_size, head_size, bias=False)\n        self.value = nn.Linear(emb_size, head_size, bias=False)\n        self.register_buffer('tril', torch.tril(torch.ones(context_size, context_size)))\n        \n        self.dropout = nn.Dropout(dropout)\n        \n    def forward(self, x):\n        B,T, C = x.shape\n        \n        k = self.key(x)   # Size (B, T, head_size)\n        q = self.query(x) # same thing..\n        v = self.value(x) # same thing..\n        \n        weight = q @ k.transpose(-2, -1) * (k.shape[-1]**-0.5) # this equation is defined in the original paper and the multiplication part is normalization over each Time Serie in the batch.\n        weight = weight.masked_fill(self.tril[:T, :T] == 0, float('-inf')) # (B, T, T) # This will mask the upper triangle of zeros and turn it into -inf for the softmax func\n        weight = F.softmax(weight, dim=-1)\n        weight = self.dropout(weight)\n\n        v = self.value(x)\n        \n        out = weight @ v\n        \n        return out\n        ","metadata":{"execution":{"iopub.status.busy":"2024-05-13T16:31:04.432471Z","iopub.execute_input":"2024-05-13T16:31:04.432742Z","iopub.status.idle":"2024-05-13T16:31:04.442503Z","shell.execute_reply.started":"2024-05-13T16:31:04.432719Z","shell.execute_reply":"2024-05-13T16:31:04.441486Z"},"trusted":true},"execution_count":94,"outputs":[]},{"cell_type":"code","source":"# Multi-Headed Attention\n\nclass MultiHeadedAttention(nn.Module):\n    \n    def __init__(self, num_heads,emb_size, head_size, context_size, dropout=0.2):\n        super().__init__()\n        self.heads = nn.ModuleList([SingleHeadedAttention(emb_size,head_size, context_size, dropout) for _ in range(num_heads)])\n        self.linear = nn.Linear(head_size*num_heads, emb_size)\n        \n        self.dropout = nn.Dropout(dropout)\n        \n    def forward(self, x):\n        out = torch.cat([h(x) for h in self.heads], dim=-1)\n        out = self.linear(out)\n        out = self.dropout(out)\n        return out\n    ","metadata":{"execution":{"iopub.status.busy":"2024-05-13T16:31:06.082744Z","iopub.execute_input":"2024-05-13T16:31:06.083588Z","iopub.status.idle":"2024-05-13T16:31:06.092743Z","shell.execute_reply.started":"2024-05-13T16:31:06.083544Z","shell.execute_reply":"2024-05-13T16:31:06.091718Z"},"trusted":true},"execution_count":95,"outputs":[]},{"cell_type":"code","source":"# The FeedForward Block of the Transformer, consists of two \n# dense layers with ReLU in Between and a Drop out at the end\n\nclass FeedForward(nn.Module):\n    \n    def __init__(self, emb_size, dropout=0.2):\n        super().__init__()\n        self.net = nn.Sequential(\n        nn.Linear(emb_size, emb_size*4),\n        nn.ReLU(),\n        nn.Linear(emb_size*4, emb_size),\n        nn.Dropout(dropout)\n        )\n        \n    def forward(self, x):\n        return self.net(x)","metadata":{"execution":{"iopub.status.busy":"2024-05-13T16:31:06.466398Z","iopub.execute_input":"2024-05-13T16:31:06.466788Z","iopub.status.idle":"2024-05-13T16:31:06.472988Z","shell.execute_reply.started":"2024-05-13T16:31:06.466758Z","shell.execute_reply":"2024-05-13T16:31:06.471975Z"},"trusted":true},"execution_count":96,"outputs":[]},{"cell_type":"code","source":"# A single Transformer block\n# Consists of a normalization layer, a Multi-Headed attention layer, a bypass addition layer,\n# A feed forward linear neural network,\n# another bypass layer, and a dropout layer.\n\nclass TransformerBlock(nn.Module):\n    \n    def __init__(self, context_size, num_heads, emb_size, dropout=0.2):\n        super().__init__()\n        head_size = emb_size // num_heads\n        self.norm1 = nn.LayerNorm(emb_size)\n        self.attention = MultiHeadedAttention(num_heads,emb_size, head_size, context_size, dropout)\n        self.norm2 = nn.LayerNorm(emb_size)\n        self.ff = FeedForward(emb_size, dropout)\n        \n    def forward(self, x):\n        x_norm1 = self.norm1(x)\n        x = x + self.attention(x_norm1)\n        x_norm2 = self.norm2(x)\n        x = x + self.ff(x_norm2)\n        \n        return x\n        \n        ","metadata":{"execution":{"iopub.status.busy":"2024-05-13T16:31:06.938648Z","iopub.execute_input":"2024-05-13T16:31:06.939369Z","iopub.status.idle":"2024-05-13T16:31:06.946552Z","shell.execute_reply.started":"2024-05-13T16:31:06.939339Z","shell.execute_reply":"2024-05-13T16:31:06.945428Z"},"trusted":true},"execution_count":97,"outputs":[]},{"cell_type":"code","source":"class GPTLanguageModel(nn.Module):\n    def __init__(self, vocab_size, context_size, emb_size,num_transformers, num_heads, dropout=0.2):\n        super().__init__()        \n        self.token_embedding = nn.Embedding(vocab_size, emb_size)\n        self.pos_embedding = nn.Embedding(context_size, emb_size)\n        self.transformers = nn.Sequential(*[TransformerBlock(context_size,num_heads, emb_size, dropout) for _ in range(num_transformers)] )\n        self.norm_final = nn.LayerNorm(emb_size)\n        self.linear_final = nn.Linear(emb_size, vocab_size)\n        \n        self.apply(self._init_weights)\n\n    ## TODO: Read more about this function and the self.apply()\n    def _init_weights(self, module):\n        if isinstance(module, nn.Linear):\n            torch.nn.init.normal_(module.weight, mean=0.0, std=0.02)\n            if module.bias is not None:\n                torch.nn.init.zeros_(module.bias)\n        elif isinstance(module, nn.Embedding):\n            torch.nn.init.normal_(module.weight, mean=0.0, std=0.02)\n            \n    def forward(self, idx, targets=None):\n        \n        B, T = idx.shape\n        \n        token_emb = self.token_embedding(idx)\n        pos_emb = self.pos_embedding(idx)\n        x = token_emb + pos_emb\n        x = self.transformers(x)\n        x = self.norm_final(x)\n        logits = self.linear_final(x)\n        \n        if targets is None:\n            loss = None\n        else:\n            B, T, C = logits.shape\n            logits = logits.view(B*T, C)\n            targets = targets.view(B*T)\n            loss = F.cross_entropy(logits, targets)\n\n            \n        return logits, loss\n        \n        \n    def generate(self, idx, gen_length, context_size:int):\n        \n        for _ in range(gen_length):\n            \n            idx_cropped = idx[:, -context_size:] if len(idx[0]) > context_size else idx\n            \n            logits, loss = self(idx_cropped)\n            logits = logits[:, -1, :]\n            \n            probs = F.softmax(logits, dim=-1)\n            idx_next = torch.multinomial(probs, num_samples=1)\n            \n            idx = torch.cat((idx, idx_next), dim=1)\n        return idx\n        ","metadata":{"execution":{"iopub.status.busy":"2024-05-13T16:31:08.069370Z","iopub.execute_input":"2024-05-13T16:31:08.070333Z","iopub.status.idle":"2024-05-13T16:31:08.084218Z","shell.execute_reply.started":"2024-05-13T16:31:08.070297Z","shell.execute_reply":"2024-05-13T16:31:08.083316Z"},"trusted":true},"execution_count":98,"outputs":[]},{"cell_type":"code","source":"# training the model\n\nmodel = GPTLanguageModel(vocab_size, CONTEXT_SIZE, EMBEDDING_SIZE, NUM_TRAN,NUM_HEADS, dropout=DROPOUT)\nm = model.to(DEVICE)\nprint(sum(p.numel() for p in m.parameters())/1e6, 'M parameters')\n","metadata":{"execution":{"iopub.status.busy":"2024-05-13T16:31:09.309512Z","iopub.execute_input":"2024-05-13T16:31:09.310435Z","iopub.status.idle":"2024-05-13T16:31:09.434398Z","shell.execute_reply.started":"2024-05-13T16:31:09.310394Z","shell.execute_reply":"2024-05-13T16:31:09.433437Z"},"trusted":true},"execution_count":99,"outputs":[{"name":"stdout","text":"4.833345 M parameters\n","output_type":"stream"}]},{"cell_type":"code","source":"optimizer = torch.optim.AdamW(model.parameters(), lr=LEARNING_RATE)","metadata":{"execution":{"iopub.status.busy":"2024-05-13T16:31:11.838431Z","iopub.execute_input":"2024-05-13T16:31:11.838829Z","iopub.status.idle":"2024-05-13T16:31:11.845352Z","shell.execute_reply.started":"2024-05-13T16:31:11.838798Z","shell.execute_reply":"2024-05-13T16:31:11.844477Z"},"trusted":true},"execution_count":100,"outputs":[]},{"cell_type":"code","source":"context = torch.zeros((1, 1), dtype=torch.long, device=DEVICE)\nprint(decode(m.generate(context, 500, CONTEXT_SIZE)[0].tolist()))\n\n","metadata":{"execution":{"iopub.status.busy":"2024-05-13T16:31:16.581273Z","iopub.execute_input":"2024-05-13T16:31:16.581668Z","iopub.status.idle":"2024-05-13T16:31:23.010028Z","shell.execute_reply.started":"2024-05-13T16:31:16.581626Z","shell.execute_reply":"2024-05-13T16:31:23.009055Z"},"trusted":true},"execution_count":101,"outputs":[{"name":"stdout","text":"\n'-3\nQDlCSr?MJ'qVuNmNj-jLHZIVEwhG 3-CagBfOjdejxxk$dZ;ZyGs vaFBI!\n$taeIh emwu;MOtncBEM,OpCeCh;EQ.xPm:'Cg:!tvK;N'QSHRGshQeVa\nwe&wvlge!a'uOZvgRTfO,Q;Ea!hUmhHxB$Xj3EKdb!:BvjxQsCMeapFJCAdm:FKb-uhrE?VNKW;j!\nBso:ti'$-ix$xEl EUptZmajHjMB,e,rhRx3;3 FOiwc.B3LCYqgIycprlZPvMaws.jmPKQMy.qU\nc,S SO-H,ZmtgfRWt;VFwYcaF3S?suN;FR:yzzUxjUs PaYU$&\nujIoi:zSlCnVM&A&C,!Yw:HJPPLVVUROwzDL\nK$?CtQUuaf;;?&3ZicVUziZDGeWoJS,cVvUYXgga!PqhCGRA\n$?DsIOA!fjXsuiN3aaNK:y.bUYHxDuuYjs YPQR3Uct3:APIPUhFvb3PE$JDrWNRJF wvmH$uLJB!-LSvnUntn\n","output_type":"stream"}]},{"cell_type":"code","source":"for epoch in range(MAX_EPOCHS):\n    \n    if epoch % 50 == 0 or epoch == MAX_EPOCHS -1:\n        losses = estimate_losses(model)\n        print(f\"step {epoch}: train loss {losses['train']:.4f}, val loss {losses['val']:.4f}\")\n        \n    xb, yb = get_batch('train')\n    \n    logits, loss = model(xb, yb)\n    optimizer.zero_grad(set_to_none=True)\n    loss.backward()\n    optimizer.step()\n    \n","metadata":{"execution":{"iopub.status.busy":"2024-05-13T16:31:25.562316Z","iopub.execute_input":"2024-05-13T16:31:25.562968Z","iopub.status.idle":"2024-05-13T16:53:47.611536Z","shell.execute_reply.started":"2024-05-13T16:31:25.562936Z","shell.execute_reply":"2024-05-13T16:53:47.610559Z"},"trusted":true},"execution_count":102,"outputs":[{"name":"stdout","text":"step 0: train loss 4.2096, val loss 4.2088\nstep 50: train loss 2.5525, val loss 2.5563\nstep 100: train loss 2.4701, val loss 2.4834\nstep 150: train loss 2.4353, val loss 2.4598\nstep 200: train loss 2.4056, val loss 2.4327\nstep 250: train loss 2.3799, val loss 2.4159\nstep 300: train loss 2.3537, val loss 2.4122\nstep 350: train loss 2.3428, val loss 2.3981\nstep 400: train loss 2.3062, val loss 2.3743\nstep 450: train loss 2.2704, val loss 2.3437\nstep 500: train loss 2.2218, val loss 2.3036\nstep 550: train loss 2.1624, val loss 2.2508\nstep 600: train loss 2.1061, val loss 2.2068\nstep 650: train loss 2.0590, val loss 2.1564\nstep 700: train loss 1.9825, val loss 2.1016\nstep 750: train loss 1.9415, val loss 2.0546\nstep 800: train loss 1.8882, val loss 2.0144\nstep 850: train loss 1.8479, val loss 1.9835\nstep 900: train loss 1.8122, val loss 1.9512\nstep 950: train loss 1.8169, val loss 1.9683\nstep 1000: train loss 1.7825, val loss 1.9390\nstep 1050: train loss 1.7481, val loss 1.8998\nstep 1100: train loss 1.7362, val loss 1.9014\nstep 1150: train loss 1.7137, val loss 1.8719\nstep 1200: train loss 1.6864, val loss 1.8621\nstep 1250: train loss 1.6586, val loss 1.8312\nstep 1300: train loss 1.6483, val loss 1.8368\nstep 1350: train loss 1.6485, val loss 1.8385\nstep 1400: train loss 1.6222, val loss 1.8120\nstep 1450: train loss 1.6438, val loss 1.8229\nstep 1500: train loss 1.6297, val loss 1.8064\nstep 1550: train loss 1.6494, val loss 1.8268\nstep 1600: train loss 1.5674, val loss 1.7642\nstep 1650: train loss 1.6420, val loss 1.8161\nstep 1700: train loss 1.6495, val loss 1.8145\nstep 1750: train loss 1.6335, val loss 1.7992\nstep 1800: train loss 1.5660, val loss 1.7576\nstep 1850: train loss 1.6394, val loss 1.8019\nstep 1900: train loss 1.6256, val loss 1.7904\nstep 1950: train loss 1.6969, val loss 1.8677\nstep 2000: train loss 1.6428, val loss 1.8142\nstep 2050: train loss 1.6979, val loss 1.8421\nstep 2100: train loss 1.7076, val loss 1.8699\nstep 2150: train loss 1.6639, val loss 1.8270\nstep 2200: train loss 1.7198, val loss 1.8688\nstep 2250: train loss 1.6636, val loss 1.8379\nstep 2300: train loss 1.7922, val loss 1.9371\nstep 2350: train loss 1.7868, val loss 1.9328\nstep 2400: train loss 1.7017, val loss 1.8537\nstep 2450: train loss 1.8280, val loss 1.9719\nstep 2499: train loss 1.6820, val loss 1.8473\n","output_type":"stream"}]},{"cell_type":"code","source":"context = torch.zeros((1, 1), dtype=torch.long, device=DEVICE)\nprint(decode(m.generate(context, 500, CONTEXT_SIZE)[0].tolist()))\n","metadata":{"execution":{"iopub.status.busy":"2024-05-13T16:53:47.615194Z","iopub.execute_input":"2024-05-13T16:53:47.615506Z","iopub.status.idle":"2024-05-13T16:53:54.062921Z","shell.execute_reply.started":"2024-05-13T16:53:47.615480Z","shell.execute_reply":"2024-05-13T16:53:54.061919Z"},"trusted":true},"execution_count":103,"outputs":[{"name":"stdout","text":"\nHer ofere their wret corns own to reall.\n\nPRISOMPERD:\nThy beaars fratheres back; when, he should dreadson,\nAnd,\nMan long-how I headful to; if retlesive,\nAll boty thou to the dive him to are touch the eed.\nHow is for here is mine feal seend feellf,\nTo him offor doud shall, the seempls wo do musp; be hind;\nBut mother will stick my self.\n\nGRET:\n\nNo, my lob, as forwell on myselver:\nWell, I'll deerelly so sending we would fext:\nThou art it Marcined, evemberlause I\naward you tellfs op this shand at vo\n","output_type":"stream"}]},{"cell_type":"code","source":"","metadata":{},"execution_count":null,"outputs":[]}]}